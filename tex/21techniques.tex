Despite the fact that the computational model of online algorithms with
advice has been only concieved a few years ago it is already possible to
notice the emergence of common techniques to analyze online problems and
find lower and upper bounds for their advice complexity.

To find an upper bound the most straightforward method is, same as with
other complexity metrics, to find an algorithm which solves the problem
and then determine its advice complexity. Any optimal algorithm cannot
then have any worse advice complexity. While this method is obvious, it is
often the most demonstrative one.

Proving lower bounds is usually significantly more difficult. Instead of
showing an algorithm which does not need more than a certain amount of
advice, to prove that $b$ is a lower bound, we need to show that any
algorithm with a certain guarantee on the competitive ratio cannot achieve
this without reading at least $b$ bits.

\subsection{Common Prefix}

Probably the most basic approach to finding the lower bound on the advice
complexity of a particular online problem is to find a set of instances
with the following properties:

\begin{enumerate}[(i)]
    \item
    for a given non-negative integer $k$ the prefixes $(x_1^{(i)}, \dots,
    x_k^{(i)})$ of instances $I^{(i)}$ are equal, i.e., for two instances
    $I^{(i)} \not= I^{(j)}$, for each $l$ such that $1 \leq l \leq k$,
    the members $x_l^{(i)}$ and $x_l^{(j)}$ are equal

    \item
    for each pair of instances $I^{(i)} \not= T^{(j)}$ there are no
    optimal solutions $Opt(I^{(i)}) = (y_1^{(i)}, \dots, y_{n_i}^{(i)})$,
    $Opt(I^{(j)}) = (y_1^{(j)}, \dots, y_{n_j}^{(j)})$ such that
    $$
        (y_1^{(i)}, \dots, y_{k}^{(i)}) = (y_1^{(j)}, \dots, y_{k}^{(j)})
    $$
\end{enumerate}

In other words, we find a set of instances such that the algorithm cannot
possibly distinguish the prefixes of these instances, however, for each
instance a unique solution needs to be yielded in the prefix already. To
achieve this, the advice string must necessarily be used. If the size of
this set of instances is $m$, at least $\lg m$ advice bits need to be
accessed which gives a lower bound on the advice complexity of the
problem.

This technique is used in various proofs in \cite{misof-trivial-graphs}
and in \cite{komm-thesis} to prove a lower bound on the advice complexity
of disjoint path allocation.

It is possible to generalize this technique to show lower bounds not only
on the advice complexity of an optimal solution, but also to show lower
bounds for $c$-competitive algorithms for a given constant $c$.

In this case, it is useful to look at an online algorithm with $b$ bits of
advice as a collection of $2^b$ deterministic online algorithms with
different strategies. If the problem in question has the property that a
strategy (sequence of decisions on the common prefix) leading to an
optimal solution for a particular instance $I$ also leads to a competitive
solution for a set of similar instances, we can estimate an upper bound on
the number of such similar instances, let us denote this by $s$. A lower
bound on the number of required strategies is then obtained as $m/c$,
which means that $\log\frac{m}{b}$ is a lower bound on the number of
advice bits.

\subsection{Reduction to String Guessing}

In \cite{string-guessing} the authors use reductions to a simpler problem
that is easier to analyze as a method to prove lower bounds. Specifically,
they picked the string guessing problem in two variants.

\begin{definition}[String Guessing with Known History]
    The string guessing problem with known history over an alphabet
    $\Sigma$ of size $q \geq 2$ (denoted as \sgkh{q}) is defined as
    follows. The input instance $I = (n, d_1, \dots, d_n)$ consists of an
    integer $n$ specifying the length of the instance and a sequence of
    $n$ characters, where $d_i \in \Sigma, 1 \leq i \leq n$. Let $A$ be an
    online algorithm that solves \sgkh{q}, then $A(I) = (y_1, \dots, y_n,
    \_)$, where $y_i \in \Sigma$. We define the cost of a solution as the
    Hamming distance between the sequence $(y_1, \dots, y_n)$ and the
    sequence $(d_1, \dots, d_n)$, i.e. the number of wrongly guessed
    characters.
\end{definition}

\begin{definition}[String Guessing with Unknown History]
    The string guessing problem with unknown history over an alphabet
    $\Sigma$ of size $q \geq 2$ (denoted as \sguh{q}) is defined as
    follows. The input instance $I = (n, ?_2, \dots, ?_n, d)$ consists of
    an integer $n$ specifying the length of the instance, $n-1$ queries
    without additional information and a string $d = d_1d_2\dots{}d_n$,
    where $d_i \in \Sigma, 1 \leq i \leq n$. Let $A$ be an online
    algorithm that solves \sguh{q}, then $A(I) = (y_1, \dots, y_n, \_)$,
    where $y_i \in \Sigma$. We define the cost of a solution as the
    Hamming distance between the sequence $(y_1, \dots, y_n)$ and the
    sequence $(d_1, \dots, d_n)$.
\end{definition}

Both \sgkh{q} and \sguh{q} consist of $n + 1$ queries where for the first
$n$ queries the algorithm is expected to guess a single character of the
instance; for the last query no meaningful response is expected, its
purpose is only to reveal the input string to allow an offline algorithm
to guess the whole string correctly. The only difference between the two
variants is that in \sgkh{q} it is revealed whether the algorithm guessed
correctly after each guess and in \sguh{q} this is revealed in the last
turn.

For the sake of simplicity, we may sometimes speak about the input string
$d = d_1d_2\dots{}d_n$ instead of the corresponding input instance $I =
(n, d_1, \dots, d_n)$ in the case of \sgkh{q} or $I = (n, ?_2, \dots, ?_n,
d)$ in the case of \sguh{q}.

It is easy to observe the following relationship between bounds for the
two variants of the string guessing problem.

\begin{observation}\label{observation:sguh-sgkh-bounds}
    Any upper bound on the advice complexity of \sguh{q} is also an upper
    bound on the advice complexity of \sgkh{q} -- any algorithm that
    solves \sguh{q} can be used to solve \sgkh{q} as well, simply ignoring
    the characters provided in each query. Similarly, any lower bound for
    \sgkh{q} is also a lower bound for \sguh{q}.
\end{observation}

With this in mind, bounds on the advice necessary to achieve optimality
for both veriants have been shown.

\begin{theorem}[\cite{string-guessing}]\label{theorem:sgkh-upper}\label{theorem:sguh-upper}
    The advice complexity of \sguh{q} is at most $\ceil{n \lg q}$.
\end{theorem}

\begin{proof}
    We prove this theorem by describing an algorithm $A$ using $\ceil{n
    \lg q}$ bits of advice which solves both \sgkh{q} and \sguh{q}.

    The total number of strings of length $n$ is $q^n$. These can be
    sorted in a lexicographic order in which each instance has a position.
    To encode this position, $\ceil{n \lg q}$ bits are required.

    Therefore, after receiving the number $n$ in the first query, $A$
    reads the position $m$ of the string from the advice string and
    enumerates the first $m$ strings of length $n$ in lexicographic order
    until it finds the correct one. Then it just yields one character from
    the string per query.
\end{proof}

\begin{theorem}[\cite{string-guessing}]\label{theorem:sgkh-lower}
    The advice complexity of \sgkh{q} is at least $\ceil{n \lg q}$.
\end{theorem}

\begin{proof}
    We prove this by contradiction. Suppose there is an algorithm $A$
    which solves \sgkh{q} using $m$ bits of advice, $m < \ceil{n \lg q}$.
    The total number of instances of length $n$ is $q^n$. However, using
    $m$ bits of advice it is possible to only encode $2^m \leq 2^{\ceil{n
    \lg q} - 1} < 2^{n \lg q} = q^n$ different values. Therefore, there
    are two input strings $d, d'$ where the same $m$-bit advice string
    $\phi$ leads to the optimal solution.

    Consider the first position $i$ at which strings $d$ and $d'$ differ,
    i.e., $d_i \not= d'_i$. Since $A$ gives the optimal result for the
    input string $d$, in the $i$-th turn it emits $d_i$. However, since up
    until the $i$-th turn, the input is the same for $d'$ as well and
    since the advice string is also the same, $A$ is in exactly the same
    state in the $i$-th turn when processing $d'$ as it is when processing
    $d$. Therefore, for the input string $d'$, $A$ outputs $d_i$ in the
    $i$-th turn as well. This contradicts the assumption that $A$ provides
    an optimal solution for $d'$.
\end{proof}

The following corollary follows from the previous two theorems and
observation \ref{observation:sguh-sgkh-bounds}.

\begin{corollary}
    The advice complexity of both \sgkh{q} and \sguh{q} is $\ceil{n \lg
    q}$.
\end{corollary}

The following lower bounds on the number of advice bits required to
guarantee that an algorithm guesses at least a certain amount of
characters right have been established.

\begin{theorem}[\cite{string-guessing}]\label{theorem:sguh-lower-ratio}\label{theorem:sgkh-lower-ratio}
    To guarantee that an online algorithm $A$ guesses at least $\alpha{}n$
    characters right for an instance of either \sguh{q} or \sgkh{q} of
    length $n$, where $\frac{1}{q} \leq \alpha < 1$, $A$ needs to access at
    least
    $$
        \paren*{1 + (1 - \alpha)\log_q\paren*{\frac{1 - \alpha}{q - 1}} +
        \alpha\log_q\alpha} n\lg{}q
        =
        (1 - H_q (1 - \alpha)) n\lg{}q
    $$
    advice bits, where $H_q$ is the $q$-ary entropy function defined as
    $$
        H_q(p) = p \log_q(q - 1) - x\log_qx - (1-x)\log_q(1-x)
    $$
    for any $q \in \N^{\geq2}, 0 \leq p \leq 1$.
\end{theorem}

Even though thanks to observation \ref{observation:sguh-sgkh-bounds} it
would suffice to show this bound for \sgkh{q}, it has been proved for each
problem independently as both proofs are interesting in their own right.

The proof for \sguh{q} uses the common prefix technique described in the
previous subsection. This is possible thanks to the fact that all
instances of length $n$ are identical except for the very last query.

In this problem, each strategy is in fact one hard-coded string of length
$n$ that an algorithm outputs for each instance. Since the output is
allowed to differ in at most $(1-\alpha)n$ characters, all instances for
which a strategy is acceptable have a Hamming distance of at most
$(1-\alpha)n$ from the guessed character. The lower bound is then obtained
by estimating the number of strings of length $n$ within the appropriate
Hamming distance.

For \sgkh{q}, however, the common prefix technique is no longer
applicable, because an algorithm receives information about the
correctness of its guess after each round. Even though this information
does not correlate with the rest of the instance in any way, an algorithm
may make different decisions based on the correctness of its previous
guesses.

The formal proof of this bound is therefore significantly more complicated
than in the \sguh{q} problem and involves representing each computation as
a walk through a complete rooted $q$-ary tree of depth $n$ and estimating
the number of instances in each subtree for which an adversary is able to
enforce at most $e$ errors. This number of instances turns out to be the
same as in the case of \sguh{q}, which leads to the same lower bound.

These results have been used in \cite{string-guessing} to establish lower
bounds on the advice required to attain a certain competitive ratio for
the online version of the maximum clique problem and the online set cover
problem.

\subsection{Partition Tree}

A generalization of the common prefix technique has been introduced in
\cite{sofsem2014}. It is not always possible to isolate enough instances
which all have the same prefix of sufficient length. However, it may be
possible to find a set of instances such that certain pairs of instances
share common prefix of some length (where the length may differ for each
pair) and, again, require different handling on this prefix.

The technique is formalized by organizing instances into a tree based on
their common prefixes.

\begin{definition}[Partition Tree]\label{definition:partition-tree}
    Consider an online problem and a set of instances $\I$ for this problem.
    We define a \emph{partition tree} $T(\I)$ of $\I$ as a labeled rooted
    tree with the following properties:
    \begin{enumerate}[(i)]
        \item Each vertex $v$ of $T(\I)$ is labeled by a non-empty set of
            instances $\I_v \subseteq \I$ and by a natural number $k_v$,
            such that any two instances $I_1, I_2 \in \I_v$ have a common
            prefix of length at least $k_v$.

        \item For each non-leaf vertex $v$, the instance sets of its
            children form a partition of $\I_v$. For each child $w$ of
            $v$, $k_w \geq k_v$.

        \item The instance set of the root of $T(\I)$ is $\I$.
    \end{enumerate}
\end{definition}

These properties ensure that if an algorithm processes two instances from
the instance set of a single vertex $v$, the algorithm cannot distinguish
these instances based on their prefix of length $k_v$. If we combine this
property with an additional one, that for any two instances belonging to
different children of $v$, different outputs for the common prefix are
required, these outputs are only determined by the advice, as the
following lemma states.

\begin{lemma}[\cite{sofsem2014}]\label{lemma:partition-tree}
    Let $\I$ be a set of instances for an online problem and let $T(\I)$
    be a partition tree of $\I$. Let $v_1, v_2$ be two different vertices
    of $T(\I)$ such that neither is an ancestor of the other and let $v$
    be the lowest common ancestor of $v_1$ and $v_2$. Let $I_1 \in
    \I_{v_1}$ and $I_2 \in \I_{v_2}$ and let $OPT(I)$ denote the set of
    optimal output sequences for any instance $I$.

    If, for all $\pi_1 \in OPT(I_1), \pi_2 \in OPT(I_2)$, $\pi_1$ and
    $\pi_2$ differ in the first $k_v$ elements, then any optimal algorithm
    needs a different advice string for each of the two instances $I_1$
    and $I_2$.
\end{lemma}

If we take, for instance, a partition tree satisfying the prerequisite of
lemma \ref{lemma:partition-tree}, and apply the lemma to its leaves, we
can see that each leaf requires a unique advice string. This observation
leads to the following theorem.

\begin{theorem}[\cite{sofsem2014}]\label{theorem:partition-tree}
    Let $\I$ be a subset of the set of all instances of an online problem
    $\onlineproblem$ and let $T(\I)$ be a partition tree of $\I$
    satisfying the prerequisite of lemma \ref{lemma:partition-tree}.

    Then, any optimal online algorithm for $\onlineproblem$ needs to read
    at least $\log{}m$ bits of advice, where $m$ is the number of leaves
    of $T(\I)$.
\end{theorem}

While the prerequisite of lemma \ref{lemma:partition-tree} might appear to
be rather difficult to prove, it is easily satisfied if we create a set of
instances $\I$ orgainzed in a partition tree $T(\I)$, such that every leaf
of $T(\I)$ contains only one instance, each instance has only one optimal
output sequence and an optimal sequence for some instance from $\I$ is not
optimal for any other instance in $\I$.

If a tree satisfies these three conditions, all that is left to show for
lemma \ref{lemma:partition-tree} to hold is that for every pair of
instances from $\I$, the output sequences differ in the first $k$ items,
where $k$ is the length of their common prefix.
