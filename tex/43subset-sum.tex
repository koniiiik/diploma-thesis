We devote the rest of this chapter to an analysis of the \emph{subset sum}
problem, which is another example of an $NP$-hard problem
\cite{subset-sum-np-hard}. This problem can be formulated as the
following 0-1 integer programming problem.

\begin{definition}[Subset Sum]\label{definition:subset-sum}
    Given a vector $\vec{a} = (a_1, \dots, a_n)$ of positive integers and
    a positive integer $M$, find a feasible solution to the 0-1 integer
    programming problem
    \begin{equation}\label{eqn:subset-sum-def}
        \sum_{i=1}^n a_ix_i = M; \quad x_i \in \{0,1\} \  \text{for all i}
    \end{equation}
\end{definition}

This problem is of interest because of its applications in cryptography:
multiple asymmetric cryptographic schemes have been proposed based on this
problem \cite{merkle-hellman, chor-rivest}. These are usually referred to
as knapsack-based cryptosystems, since subset sum is a special case of the
0-1 knapsack problem. In these schemes, the public key generally consists
of a set of weights $\{a_i \mid 1 \leq i \leq n\}$ and ciphertext is
obtained from a plaintext binary message $(b_1, \dots, b_n)$ as
$\sum_{i=1}^n a_ib_i$. The private key then consists of additional
information which makes it possible to solve instances of the subset sum
problem with the given sequence of $a_i$ in polynomial time.

First we present some rudimentary properties of the advice required to
solve subset sum and other $NP$-hard problems in polynomial time and then
we look at a more sophisticated algorithm which applies to a specific
class of subset sum instances.

\subsection{General Results}
\label{section:subset-sum-general}

A nondeterministic algorithm for the subset sum problem might guess the
values of variables $x_i$ in \eqref{eqn:subset-sum-def} and verify that
the guess is correct. This suggests a trivial algorithm with advice: it
simply reads the values of $x_i$ from the advice tape, writes them to the
output and finishes. Obviously, there is nothing interesting about this
algorithm, since it reads the whole output from the advice.

A slightly better algorithm is obtained if we do not read the whole output
from advice, but instead stop after reading the $k$-th bit, i.e. $A$ now
knows the values of the first $k$ variables $x_i$. Then, our algorithm can
find the correct values for the remaining $n-k$ varibles $x_i$ by
exhaustive search, which takes $O(2^{n-k})$ time. This observation leads
to the following claim.

\begin{theorem}\label{theorem:subset-sum-advice-upper}
    The subset sum problem can be solved in polynomial time with
    $n-O(\log{}n)$ bits of advice.
\end{theorem}

\begin{proof}
    Let $f(n) \leq c\log{}n$ for some $c$, let $k = n - c \log{}n$. $A$
    reads $k$ bits of advice and interprets them as the values of $x_1,
    \dots, x_k$ from \eqref{eqn:subset-sum-def}. $A$ then performs an
    exhaustive search for the remaining $c \log{}n$ values $x_{k+1},
    \dots, x_{n}$, which takes $O(P(n) \cdot 2^{c\log{}n}) = O(P(n) \cdot
    n^c)$, where $P(n)$ is a polynomial bound on the time required to
    verify the correctness of a single vector $(x_1, \dots, x_n)$.
\end{proof}

This result can be generalized to any problem from $NP$. This class of
problems can be characterized as those where for each input instance, a
polynomial-length certificate exists such that the correctness of the
certificate can be verified in polynomial time \cite{introduction}.  If
the upper bound on the length of the certificate for a given problem
$\onlineproblem$ is $c(n)$, the sufficient advice to solve
$\onlineproblem$ is $c(n) - O(\log{}n)$.

We can establish a lower bound on the amount of advice for any $NP$-hard
problem in a similar way: If we can solve some $NP$-hard problem using
$O(\log{}n)$ bits of advice, it is easy to perform an exhaustive search in
polynomial time for the right advice string using a deterministic Turing
machine.

\begin{theorem}\label{theorem:np-hard-advice-lower}
    For any $NP$-hard problem $\onlineproblem$, the amount of advice
    required to solve $\onlineproblem$ is $\omega(\log{}n)$, unless
    $P=NP$.
\end{theorem}

\subsection{Lattice-Based Algorithm for Low-Density Subset Sum}
\label{section:lattice-based}

Lagarias and Odlyzko proposed in \cite{lagarias-odlyzko} an algorithm
called SV (Short Vector) which solves almost all instances of subset sum
with a low density. The density of a vector $\vec{a}$ of $n$ elements is
defined as
$$
    d(\vec{a}) = \frac{n}{\log\max\limits_ia_i}.
$$
Informally, instances with a low density are those consisting of elements
significantly larger than $2^n$.

We formulate the following observation which clarifies the importance of
this restricted version of the subset sum problem.

\begin{observation}\label{observation:subset-sum-low-density-reduction}
    If there is an algorithm $A$ for solving all instances $(\vec{a}, M)$
    of subset sum such that $d(a) < c$ for some $c \in \R^+$ in polynomial
    time, we can solve any instance of subset sum in polynomial time,
    regardless of density.
\end{observation}

\begin{proof}
    Assume we can solve all instances with density lower than $c$ for some
    $c > 0$. Consider an instance $(\vec{a}, M)$ such that $d(\vec{a}) >
    c$. Let $e = n/c - \log\max_ia_i$. From $d(\vec{a}) > c$ follows that
    $e > 0$.
    
    If we multiply both $\vec{a}$ and $M$ by $2^{\ceil{e}+1}$, we obtain
    an instance $(\vec{a}', M')$ with identical solutions to those of
    $(\vec{a}, M)$.

    For the density of $\vec{a}'$, the bound
    \begin{multline*}
        d(\vec{a}')
        = \frac{n}{\log\paren*{2^{\ceil{e}+1} \max\limits_ia_i}}
        \leq \frac{n}{e + 1 + \log\max\limits_ia_i}
        \\
        = \frac{n}{1 + \frac{n}{c} - \log\max\limits_ia_i + \log\max\limits_ia_i}
        = \frac{cn}{c + n}
        = c \cdot \paren*{1 - \frac{c}{n+c}} < c
    \end{multline*}
    holds, which means $(\vec{a}', M')$ can be solved in polynomial time
    in the size of the new instance.

    To be entirely correct, we note that the number $e$ is polynomial in
    $n$ and the size of a binary representation of $(\vec{a}', M')$ is $e$
    times larger than the binary representation of $(\vec{a}, M)$,
    thus this transformation keeps the size of the input polynomial.
\end{proof}

This observation implies that restricting the density does not make the
problem significantly easier to solve than it is in the general case.
Nevertheless, it does not mean we cannot use any special properties of
low-density instances to our advantage.

In the rest of this section, we focus on an analysis of the
Lagarias-Odlyzko algorithm. The ultimate goal of this analysis is to
better understand the class of instances for which this algorithm fails to
find a solution. We believe this should, in theory, make it possible to
use less advice for low-density instances, since we can tailor the advice
string specifically to the failing instances.

Unfortunately, we have not managed to obtain a sufficient characterization
of this class. We describe the results of our statistical analysis of this
algorithm and draw some conclusions in hopes they will be useful for
future research.

The SV algorithm reduces the problem of finding a solution for
\eqref{eqn:subset-sum-def} to the problem of finding the shortest vector
in a $n+1$-dimensional lattice with Euclidean norm. This problem is known
to be $NP$-hard for the supremum norm and this is conjectured to hold for
the Euclidean norm as well \cite{van-emde-boas-shortest-vector}.
Nevertheless, a polynomial algorithm for finding short (albeit not
necessarily the sortest) vectors in a lattice is due to Lenstra, Lenstra
and LovÃ¡sz \cite{LLL}, commonly referred to as LLL.

The output of the LLL algorithm is a $y$-reduced basis, where $1/4 \leq y
< 1$ is a parameter whose value, unless noted otherwise, is usually $3/4$,
containing at least one relatively short vector.

\begin{theorem}[\cite{LLL}]\label{theorem:LLL-short-vector}
    Let $[\vec{v_1}, \dots, \vec{v_n}]$ be a $y$-reduced basis of a
    lattice $L$. Then
    \begin{equation}\label{eqn:lll-y-reduced}
        |\vec{v_1}|^2 \leq \paren*{\frac{4}{4y-1}}^{n-1}
        \min\limits_{\vec{x} \in L, \vec{x} \not= \vec{0}} |\vec{x}|^2,
    \end{equation}
    where $|\vec{x}|$ denotes the Euclidean norm of $\vec{x}$.

    Specifically, for $y = 3/4$,
    \begin{equation}\label{eqn:lll-3/4-reduced}
        |\vec{v_1}|^2 \leq 2^{n-1} \min\limits_{\vec{x} \in L, \vec{x}
        \not= \vec{0}} |\vec{x}|^2.
    \end{equation}
\end{theorem}

The SV algorithm works as follows.

\begin{algorithm}[SV, \cite{lagarias-odlyzko}]\label{algorithm:sv}
    Let $\vec{a} = (a_1, \dots, a_n), M$ be the input.
    \begin{enumerate}
        \item
        Use the following vectors as a basis $[\vec{b_1}, \dots,
        \vec{b_{n+1}}]$ of an $n+1$-dimensional integer lattice
        $L(\vec{a}, M)$:
        \begin{align*}
            \vec{b_1} &= (1, 0, \dots, 0, -a_1) \\
            \vec{b_2} &= (0, 1, \dots, 0, -a_2) \\
            &\ \ \vdots \numberthis\label{eqn:sv-lll-basis} \\
            \vec{b_n} &= (0, 0, \dots, 0, -a_n) \\
            \vec{b_{n+1}} &= (0, 0, \dots, 0, M).
        \end{align*}

        \item
        Find a reduced basis $[\vec{b_1}^*, \dots, \vec{b_{n+1}}^*]$ of
        $L(\vec{a}, M)$ using the LLL algorithm.

        \item
        Check if any $\vec{b_i}^* = (b_{i,1}^*, \dots, b_{i,n+1}^*)$ has
        all $b_{i,j}^* \in \{0, \lambda\}$ for some fixed $\lambda$ for $1
        \leq j \leq n$. For any such $\vec{b_i}^*$, check if $x_j =
        \frac{1}{\lambda}b_{i,j}^*$ gives a solution to
        \eqref{eqn:subset-sum-def} and if so, halt, otherwise continue.

        \item
        Repeat steps 1 through 3 with $M$ replaced by $M' = \sum_{i=1}^n
        a_i - M$, then halt.
    \end{enumerate}
\end{algorithm}

The intuition behind this algorithm is that if a vector $\vec{e}$ is a
solution to \eqref{eqn:subset-sum-def}, then the vector $\sum_{i=1}^n e_i
\vec{b_i}$ has $-M$ on the last position and $\vec{e}$ on the first $n$
positions; adding the last vector eliminates the last position. Since the
solution vector is binary, it should presumably be one of the shortest
vectors in $L(\vec{a}, M)$.

The SV algorithm has been analyzed thoroughly and it has been shown that
the solution to \eqref{eqn:subset-sum-def} is indeed the shortest nonzero
vector in the lattice spanned by vectors $b_i$ from
\eqref{eqn:sv-lll-basis} for almost all instances such that $d(\vec{a}) <
0.645\dots$ \cite{lagarias-odlyzko, sv-improved}. As stated earlier,
though, the LLL algorithm does not guarantee finding the shortest vector,
which further decreases the chance of success of SV.

We have implemented the SV algorithm using an implementation of LLL from
the NTL library \cite{ntl}. We then tested this implementation on randomly
generated instances of subset sum with varying parameters. We have
analyzed the results to find characteristics of those instances that the
SV algorithm cannot solve.

In fact, Lagarias and Odlyzko state one possible reason for failure of the
SV algorithm in item (4) in the discussion at the end of 
\cite{lagarias-odlyzko}. If the vector $\vec{a}$ contains many linear
dependencies
$$
    \sum_{i=0}^n a_i \lambda_i = 0
$$
with $|\vec{\lambda}|$ small, the lattice $L(\vec{a}, M)$ will contain the
short vector $(\lambda_1, \dots, \lambda_n, 0)$ for all $M$. If there are
a lot of these small linear dependencies, the lattice reduction algorithm
may find those instead of the sought solution vector.

The two important parameters we varied are the dimension $n$ and the
density of generated instances. For dimensions, we considered $10, 20,
\dots, 70$, and for densities, we tried $0.3, 0.4, 0.5, 0.6$, and $0.645$.
Furthermore, to verify the hypothesis about small linear dependencies, we
generated vectors $\vec{a}$ using a fully random strategy, forcing at
least one small linear dependency and forcing at least five linear
dependencies.

For each combination of these parameters, we generated 100 random vectors
$\vec{a}$ and for each such vector, we sampled 100 random nontrivial
subsets (i.e., neither the whole $\vec{a}$, nor the empty set).
