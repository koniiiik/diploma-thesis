When proving lower bounds on the competitiveness of an online problem, it
is often useful to model instances on which an online algorithm computes
the worst solution. The concept of an \emph{adversary}, denoted by $Adv$,
does precisely that.

A computation of an online algorithm can be thought of as a game in which
there are two players: the online algorithm, trying to compute the best
solution possible, and an adversary which tries to coerce the algorithm
into making as bad decisions as possible by using information about the
decisions of the algorithm to construct an instance that is as difficult
for the algorithm to solve as possible.

For deterministic online algorithms, informally, the two entities take
turns -- the adversary submits the first part of the input and the online
algorithm provides its first result. Then, the adversary can decide how
best to construct the next part of the input instance in order to keep the
cost of the solution as far from the optimum as possible.

More formally, we define $Adv$ as an offline algorithm with knowledge of
how an algorithm $A$ works in the sense that $Adv$ is able to simulate
$A$, making it possible to anticipate every reaction $A$ makes. The output
of $Adv$ is then an instance which is used as the input for $A$.

If we can show that given an online problem $\onlineproblem$, there is an
adversary $Adv$ such that for every algorithm $A$, $Adv$ is able to
construct an instance for which $A$ fails to be $c$-competitive, that
means there is no $c$-competitive algorithm for $\onlineproblem$.

For randomized online algorithms, there are multiple definitions of
adversaries \cite{adversaries}: the oblivious adversary, the adaptive
online adversary and the adaptive offline adversary. The oblivious
adversary works in the same way as described for offline algorithms -- it
can only simulate $A$ without any information about the random data based
on which $A$ may make decisions. In the adaptive online model, $A$ and
$Adv$ play the game described earlier and $Adv$ creates the input for $A$
in an online fashion. In other words, $Adv$ knows the results of the
previous decisions of $A$ when constructing the next piece of input.
Finally, the offline adaptive adversary is omniscient -- it has full
information about the source of randomness based on which $A$ makes its
decisions.

When dealing with algorithms with advice, we need to consider whether to
allow an adversary to access the advice or not. In this thesis, we follow
the model from \cite{komm-thesis}, which gives $Adv$ full information
about the advice string corresponding to an instance it creates.

The rationale is that we usually show the existence of an adversary for
online algorithms using at most $b(n)$ bits of advice as a way of proving
a lower bound of $b(n)$ bits on the advice complexity. This can be done by
showing that for each pair $(A, O)$, where $A$ is an online algorithm and
$O$ is an oracle which computes the advice string for $A$, there is an
adversary $Adv$ which forces $A$ to fail some criterion, e.g. optimality,
or competitiveness.

We can thus assume when constructing $Adv$ that the advice does not exceed
$b(n)$ bits. Since we do not impose any restrictions on the computational
power of $A$, $Adv$, or $O$, $Adv$ can easily simulate the algorithm $A$
it is working against with all of the $2^{b(n)}$ possible advice strings
and find out which one leads to the best outcome. We can then simply
assume that $Adv$ knows which advice string is the best one for a given
instance.

The previous idea suggests a slightly different approach. By choosing a
fixed advice string $\phi$, an online algorithm becomes fully
deterministic. Thus an algorithm with $b$ bits of advice can be viewed
as a collection of $2^b$ deterministic algorithms. Showing that for any
collection of $2^b$ deterministic algorithms, there is an adversary which
forces each of them to compute a bad output is therefore equivalent to
showing that for each algorithm with $b$ bits of advice, there is such an
adversary.
